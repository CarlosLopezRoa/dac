{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()\n",
    "import pickle \n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import itertools\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sla-C4jZtYtk"
   },
   "outputs": [],
   "source": [
    "#! head -n 10000000 train.csv > traintrim.csv\n",
    "#! head -n 10000 train.txt > traintrim.txt\n",
    "#! head -n 10000000 test.csv > testtrim.csv\n",
    "#! head -n 10000 test.txt > testtrim.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_column(col, df):\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    #small_vals = train.groupby(col).count()[0].where(lambda x: x <= 1).dropna().apply(lambda x: '1').to_dict()\n",
    "    #train.iloc[:,col] = train.iloc[:,col].apply(lambda x : small_vals.get(x,x))\n",
    "    set_ = df.loc[:, col].values\n",
    "    c = Counter(set_)\n",
    "    small_vals = dict(zip(list(dict(filter(lambda x: x[1] <= 5, c.most_common())).keys()), itertools.repeat('1') ))\n",
    "    df.loc[:,col] = df.loc[:,col].apply(lambda x : small_vals.get(x,x))\n",
    "    encoder.fit(df.loc[:, col].dropna().values)\n",
    "    return encoder\n",
    "def transform_column(col, df):\n",
    "    encoder = encoders[col]\n",
    "    return encoder.transform(df.loc[:, col].dropna().values)\n",
    "def encode_test_column(col, df): \n",
    "    encoder = encoders[col+1]\n",
    "    dic = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "    return df.loc[df.loc[:, col].dropna().index, col].map(dic).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = True\n",
    "retrain = False\n",
    "retraintest = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    # For item i in a range that is a length of l,\n",
    "    for i in range(0, len(l), n):\n",
    "        # Create an index range for l of n items:\n",
    "        yield l[i:i+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if preprocess: \n",
    "    print('Import')\n",
    "    train_gen = pd.read_csv(\"train.txt\", sep='\\t', lineterminator='\\n', header=None, engine='c', chunksize = 100000)\n",
    "    train = pd.concat([chunk for chunk in tqdm(train_gen)])\n",
    "    test_gen = pd.read_csv(\"test.txt\", sep='\\t', lineterminator='\\n', header=None, engine='c', chunksize = 100000)\n",
    "    test = pd.concat([chunk for chunk in tqdm(test_gen)])\n",
    "    print(np.mean((train.count()/len(train)).values), np.mean((test.count()/len(test)).values))\n",
    "    print('Transform')\n",
    "    encoders = {x: encode_column(x, train) for x in tqdm(train.loc[:, train.columns > 13].columns)}\n",
    "    transformed_cols = {x: transform_column(x, train) for x in  tqdm(train.loc[:, train.columns > 13].columns)}\n",
    "    for col in tqdm(train.loc[:, train.columns > 13].columns):\n",
    "        train.loc[train.loc[:, col].dropna().index, col] = transformed_cols[col]\n",
    "    print('Learn')\n",
    "    predictors = dict()\n",
    "    not_nan_cols_dict = dict()\n",
    "    for col in tqdm(train.drop(0,axis=1).columns):\n",
    "        not_nan_cols = train.drop(0,axis=1).loc[train.loc[:, col].isna()].count()/len(train.loc[train.loc[:, col].isna()]) > .80\n",
    "        not_nan_cols_dict[col] = list(train.drop(0,axis=1).loc[:,not_nan_cols].columns)\n",
    "        train_nonan = train.loc[:, np.append(np.array(not_nan_cols_dict[col]), col)].dropna()#.drop(0,axis=1)\n",
    "        if (len(train_nonan.drop(col, axis = 1).values[0]) > 0) & retrain:\n",
    "            x_nonan = train_nonan.drop(col, axis = 1).values\n",
    "            y_nonan = train.loc[train_nonan.index, col].values\n",
    "            #splitter = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "            #for train_index, test_index in splitter.split(x_nonan, y_nonan):\n",
    "            #    x_train_nonan, x_test_nonan = x_nonan.loc[train_index].values, x_nonan.loc[test_index].values\n",
    "            #    y_train_nonan, y_test_nonan = y_nonan.loc[train_index].values, y_nonan.loc[test_index].values\n",
    "            if col in list(range(14)):\n",
    "                predictors[col] = RandomForestRegressor(max_depth=3, random_state=0, n_estimators=50, n_jobs=10).fit(x_nonan, y_nonan)\n",
    "            elif col in list(range(14,40)):\n",
    "                y_nonan=y_nonan.astype('int')\n",
    "                predictors[col] = RandomForestClassifier(n_estimators=50, max_depth=3, random_state=0, n_jobs=10).fit(x_nonan, y_nonan)\n",
    "    if retrain:\n",
    "        with open(\"predictorstrain.p\",\"wb\") as filehandler:\n",
    "            pickle.dump(predictors, filehandler, protocol=4)\n",
    "    print('Predict')\n",
    "    with open(\"predictorstrain.p\",\"rb\") as filehandler:\n",
    "        predictors = pickle.load(filehandler)\n",
    "    for col in tqdm(predictors.keys()):\n",
    "        if col < 16 :\n",
    "            print(col)\n",
    "            not_nan_col_lines = train.loc[train.loc[:,col].isna(), not_nan_cols_dict[col]].dropna()\n",
    "            if col in list(range(14)):\n",
    "                train.loc[not_nan_col_lines.index, col] = predictors[col].predict(not_nan_col_lines)\n",
    "            else:\n",
    "                for index in tqdm(chunks(not_nan_col_lines.index, 100000)):\n",
    "                    train.loc[index, col] = predictors[col].predict(not_nan_col_lines.loc[index,:].values)\n",
    "    print(np.mean((train.count()/len(train)).values), np.mean((test.count()/len(test)).values))\n",
    "    print('Transform')\n",
    "    transformed_test_cols = {x: encode_test_column(x, test) for x in tqdm(test.loc[:, test.columns > 12].columns)}\n",
    "    for col in tqdm(range(13,39)):\n",
    "        test.loc[test.loc[:, col].dropna().index, col] = transformed_test_cols[col]\n",
    "    print('Learn')\n",
    "    predictors = dict()\n",
    "    not_nan_cols_dict = dict()\n",
    "    for col in tqdm(test.columns):\n",
    "        not_nan_cols = test.loc[test.loc[:, col].isna()].count()/len(test.loc[test.loc[:, col].isna()]) > .80\n",
    "        not_nan_cols_dict[col] = list(test.loc[:,not_nan_cols].columns)\n",
    "        test_nonan = test.loc[:, np.append(np.array(not_nan_cols_dict[col]), col)].dropna()#.drop(0,axis=1)\n",
    "        if (len(test_nonan.drop(col, axis = 1).values[0]) > 0) & retraintest:\n",
    "            x_nonan = test_nonan.drop(col, axis = 1).values\n",
    "            y_nonan = test.loc[test_nonan.index, col].values\n",
    "            #splitter = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "            #for train_index, test_index in splitter.split(x_nonan, y_nonan):\n",
    "            #    x_train_nonan, x_test_nonan = x_nonan.loc[train_index].values, x_nonan.loc[test_index].values\n",
    "            #    y_train_nonan, y_test_nonan = y_nonan.loc[train_index].values, y_nonan.loc[test_index].values\n",
    "            if col in list(range(13)):\n",
    "                predictors[col] = RandomForestRegressor(max_depth=3, random_state=0, n_estimators=50, n_jobs=10).fit(x_nonan, y_nonan)\n",
    "            elif col in list(range(13,40)):\n",
    "                y_nonan=y_nonan.astype('int')\n",
    "                predictors[col] = RandomForestClassifier(n_estimators=50, max_depth=3, random_state=0, n_jobs=10).fit(x_nonan, y_nonan)\n",
    "    if retrain:\n",
    "        with open(\"predictorstest.p\",\"wb\") as filehandler:\n",
    "            pickle.dump(predictors, filehandler, protocol=4)\n",
    "    print('Predict')\n",
    "    with open(\"predictorstest.p\",\"rb\") as filehandler:\n",
    "        predictors = pickle.load(filehandler)\n",
    "    for col in tqdm(predictors.keys()):\n",
    "        if col not in [15,24,38]:\n",
    "            print(col)\n",
    "            not_nan_col_lines = test.loc[test.loc[:,col].isna(), not_nan_cols_dict[col]].dropna()\n",
    "            if col in list(range(13)):\n",
    "                test.loc[not_nan_col_lines.index, col] = predictors[col].predict(not_nan_col_lines)\n",
    "            else:\n",
    "                for index in tqdm(chunks(not_nan_col_lines.index, 100000)):\n",
    "                    test.loc[index, col] = predictors[col].predict(not_nan_col_lines.loc[index,:].values)\n",
    "    print(np.mean((train.count()/len(train)).values), np.mean((test.count()/len(test)).values))\n",
    "    print('filna')\n",
    "    train = train.fillna(0)\n",
    "    test = test.fillna(0)\n",
    "    print(np.mean((train.count()/len(train)).values), np.mean((test.count()/len(test)).values))\n",
    "\n",
    "    print('Export')\n",
    "    \n",
    "    train.to_csv('train.csv', index=None, header=False)\n",
    "    test.to_csv('test.csv', index=None, header=False)\n",
    "    \n",
    "    #with open(\"train.p\",\"wb\") as filehandler:\n",
    "    #    pickle.dump(train, filehandler, protocol=4)\n",
    "    \n",
    "    #with open(\"test.p\",\"wb\") as filehandler: \n",
    "    #    pickle.dump(test, filehandler, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Import preprocessed CSV\")\n",
    "train_gen = pd.read_csv(\"train.csv\",  header=None, engine='c', chunksize = 100000)\n",
    "train = pd.concat([chunk for chunk in tqdm(train_gen)])\n",
    "test_gen = pd.read_csv(\"test.csv\", header=None, engine='c', chunksize = 100000)\n",
    "test = pd.concat([chunk for chunk in tqdm(test_gen)]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gb59HDfNXYxL"
   },
   "outputs": [],
   "source": [
    "print(\"split\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "X = train.drop(0, axis = 1)\n",
    "y = train[0]#.values.reshape([-1,1])\n",
    "#enc = OneHotEncoder(sparse=False)\n",
    "#y = enc.fit_transform(y)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "splitter = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "for train_index, test_index in splitter.split(X, y):\n",
    "    X_train, X_test = X.loc[train_index].values, X.loc[test_index].values\n",
    "    y_train, y_test = y.loc[train_index].values, y.loc[test_index].values\n",
    "    \n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ub14cQv1XjOM"
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FvTbDpZJX91t"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(X.shape[1],)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "\n",
    "model.compile(tf.keras.optimizers.Adam(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = class_weight.compute_class_weight('balanced',\n",
    "                                            np.unique(y_train),\n",
    "                                            y_train)\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"training\"+now+\"/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_path, verbose=1, save_weights_only=True,\n",
    "    # Save weights, every 5-epochs.\n",
    "    period=5)\n",
    "early_cp = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0.0001,\n",
    "                              patience=15,\n",
    "                              verbose=1, mode='auto', restore_best_weights=True)\n",
    "tboard_cp = keras.callbacks.TensorBoard(log_dir='./Graph/'+now, histogram_freq=0,  \n",
    "          write_graph=True, write_images=True)\n",
    "model.save_weights(checkpoint_path.format(epoch=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxMeQn8-YLU7"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=200, \n",
    "                    batch_size=512,  verbose=1,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks = [cp_callback, early_cp, tboard_cp],\n",
    "\t\t    class_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3KcBvMmvfGhK"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CIy6QVbCiQ1k"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history_dict['acc']\n",
    "val_acc = history_dict['val_acc']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig('val_loss.png', bbox_inches='tight')\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.close()\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('acc')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.savefig('val_acc.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#sns.distplot(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = pd.DataFrame(model.predict(X_test)).apply(lambda x: int(round(x)), axis = 1).to_frame()\n",
    "y_test_pred.groupby(0)[0].count()#/y_test_pred.groupby(0)[1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(zip(list(range(60000000, 60000000+len(y_pred))), y_pred.reshape([1,-1]).tolist()[0])), columns=[\"Id\",\"Predicted\"]).to_csv(str(int(round(results[1]*1000)))+'submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Rec.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
